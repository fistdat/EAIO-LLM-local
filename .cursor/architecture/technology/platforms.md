# EAIO Technology Platform Architecture
**Architecture Mode (A.*) - Hybrid LLM Technology Stack with BDG2 Integration**

## ðŸŽ¯ Executive Summary

The EAIO technology platform supports **hybrid LLM deployment** combining **local privacy-first models** with **external API services** (ChatGPT, DeepSeek, Gemini), optimized for **MacBook Pro M1 hardware** while providing enterprise-scale energy management capabilities with flexible AI service selection.

## ðŸ§  Hybrid LLM Infrastructure

### LLM Service Architecture Strategy
```yaml
Hybrid LLM Deployment:
  Primary Strategy: Local-first with API fallback
  Privacy Levels: 
    - PRIVATE: Local LLM only (sensitive building data)
    - ENHANCED: External API allowed (aggregated insights)
    - PUBLIC: External API preferred (general analysis)
  
Cost Optimization:
  - Local LLM: Zero per-token cost, hardware investment
  - External APIs: Pay-per-use, premium capabilities
  - Intelligent routing based on query complexity and privacy
```

### ðŸ  Local LLM Stack (Primary)

#### Model Selection Matrix
| Model | Size | RAM Usage | Tokens/sec | Use Case | Privacy Level |
|-------|------|-----------|------------|----------|---------------|
| **Llama-3.2-3B-Instruct** | 3B | 3GB | 45-60 | Real-time ops, private data | HIGH |
| **Qwen2.5-7B-Instruct** | 7B | 7GB | 25-35 | Complex analysis, planning | HIGH |
| **DeepSeek-Coder-6.7B** | 6.7B | 6GB | 30-40 | Code generation, debugging | HIGH |
| **Phi-3-Mini-4K** | 3.8B | 4GB | 50-65 | Fast responses, simple tasks | HIGH |

#### Local Infrastructure
```yaml
Primary Inference Engine: Ollama v0.3+
  Advantages:
    - Native M1 optimization
    - GGUF format support
    - Built-in model management
    - OpenAI-compatible API
    - Zero external data sharing
    
Performance Characteristics:
  - Cold start: 5-15 seconds
  - Warm inference: 20-100 tokens/sec
  - Context window: 4K-32K tokens
  - Concurrent requests: 1-2 (M1 16GB limitation)
    
Privacy Benefits:
  - 100% local processing
  - No data transmission
  - Complete control over model versions
  - Regulatory compliance (GDPR, CCPA)
```

### ðŸŒ External LLM API Integration

#### Supported API Providers
```yaml
Tier 1 Providers (Production Ready):
  OpenAI:
    Models: 
      - gpt-4o: Advanced reasoning, multimodal
      - gpt-4o-mini: Cost-effective, fast
      - gpt-3.5-turbo: High-speed, affordable
    API: REST + Streaming
    Cost: $0.50-$60 per 1M tokens
    Use Cases: Complex analysis, creative solutions
    
  DeepSeek API:
    Models:
      - deepseek-chat: General purpose, cost-effective
      - deepseek-coder: Specialized code generation
      - deepseek-reasoning: Advanced logical reasoning
    API: OpenAI-compatible REST
    Cost: $0.14-$2.00 per 1M tokens
    Use Cases: Technical analysis, energy algorithms
    
  Google Gemini:
    Models:
      - gemini-1.5-pro: Advanced reasoning, large context
      - gemini-1.5-flash: Fast responses, multimodal
      - gemini-2.0-flash-exp: Latest experimental features
    API: REST with Vertex AI integration
    Cost: $1.25-$7.00 per 1M tokens
    Use Cases: Data analysis, pattern recognition

Tier 2 Providers (Future Integration):
  Anthropic Claude: 
    - claude-3.5-sonnet: Excellent for technical writing
    - claude-3-haiku: Fast, cost-effective
  
  Mistral AI:
    - mistral-large: European data compliance
    - mistral-medium: Balanced performance/cost
```

#### API Integration Architecture
```python
# Hybrid LLM Service Router
class HybridLLMService:
    def __init__(self):
        self.local_ollama = OllamaClient("http://localhost:11434")
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.deepseek_client = OpenAI(
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url="https://api.deepseek.com"
        )
        self.gemini_client = genai.GenerativeModel('gemini-1.5-pro')
        
    async def route_request(self, query: LLMRequest) -> LLMResponse:
        """Intelligent routing based on privacy, complexity, and cost"""
        
        # Privacy-first routing
        if query.privacy_level == "PRIVATE":
            return await self.local_ollama.generate(query)
            
        # Complexity-based routing
        if query.complexity_score > 0.8:
            if query.domain == "technical":
                return await self.deepseek_client.generate(query)
            elif query.domain == "analysis":
                return await self.gemini_client.generate(query)
            else:
                return await self.openai_client.generate(query)
                
        # Cost-optimized routing
        if query.cost_sensitivity == "low":
            return await self.local_ollama.generate(query)
        else:
            return await self.select_cheapest_api(query)
            
    async def fallback_strategy(self, query: LLMRequest, failed_service: str):
        """Fallback when primary service fails"""
        if failed_service == "local":
            return await self.deepseek_client.generate(query)  # Most cost-effective
        else:
            return await self.local_ollama.generate(query)  # Always available
```

### ðŸ”„ LLM Service Selection Strategy

#### Decision Matrix
```yaml
Query Classification:
  Building Data Analysis:
    Privacy: HIGH â†’ Local LLM only
    Complexity: MEDIUM â†’ Qwen2.5-7B local
    Fallback: DeepSeek API for complex algorithms
    
  Energy Optimization Planning:
    Privacy: MEDIUM â†’ Local preferred, API allowed
    Complexity: HIGH â†’ GPT-4o or Gemini-1.5-pro
    Cost Consideration: DeepSeek for technical calculations
    
  General User Queries:
    Privacy: LOW â†’ API preferred for best experience
    Response Speed: HIGH â†’ GPT-3.5-turbo or local Phi-3
    
  Code Generation:
    Privacy: MEDIUM â†’ DeepSeek-Coder local or API
    Complexity: Variable â†’ Route based on code complexity
    
Performance Requirements:
  Real-time Monitoring: Local LLM only (<200ms)
  Strategic Planning: External API allowed (<5s)
  Batch Analysis: External API preferred (cost-effective)
  Emergency Response: Local LLM only (reliability)
```

### ðŸ” Security & Privacy Framework

#### Data Classification & Routing
```yaml
Data Sensitivity Levels:
  CRITICAL (Local Only):
    - Individual building energy consumption
    - Specific equipment performance data
    - Proprietary optimization algorithms
    - Real-time operational status
    
  SENSITIVE (Local Preferred):
    - Aggregated building performance
    - Anonymous pattern analysis
    - General optimization strategies
    - Benchmarking comparisons
    
  PUBLIC (API Allowed):
    - Industry best practices research
    - General energy efficiency concepts
    - Weather correlation analysis
    - Academic research insights

Privacy Protection Measures:
  - Automatic data anonymization for API calls
  - Configurable privacy levels per building/client
  - Audit logging for all external API usage
  - Zero-trust principle for sensitive data
```

#### API Security Implementation
```python
# Secure API Integration
class SecureAPIClient:
    def __init__(self, provider: str):
        self.provider = provider
        self.encryption = Fernet(os.getenv(f"{provider}_ENCRYPTION_KEY"))
        self.rate_limiter = AsyncLimiter(max_rate=100, time_period=60)
        
    async def secure_call(self, request: LLMRequest) -> LLMResponse:
        # Data sanitization
        sanitized_request = self.sanitize_sensitive_data(request)
        
        # Rate limiting
        async with self.rate_limiter:
            # Encrypted transmission
            encrypted_payload = self.encryption.encrypt(
                json.dumps(sanitized_request.dict()).encode()
            )
            
            # API call with retry logic
            response = await self.call_with_retry(encrypted_payload)
            
            # Response validation
            validated_response = self.validate_response(response)
            
            # Audit logging
            await self.log_api_usage(request.id, self.provider, response.tokens_used)
            
            return validated_response
```

## ðŸ’° Cost Optimization Strategy

### Hybrid Cost Model
```yaml
Local LLM Costs:
  Hardware Investment: $2,000-4,000 (M1 MacBook Pro)
  Electricity: ~$0.001 per query (negligible)
  Maintenance: Software updates only
  Scaling: Linear hardware investment
  
External API Costs (Per Month Estimates):
  Light Usage (1K queries): $10-50
  Medium Usage (10K queries): $100-500  
  Heavy Usage (100K queries): $1,000-5,000
  Enterprise Usage (1M queries): $10,000-50,000
  
Cost Optimization Rules:
  - Simple queries â†’ Local LLM (free)
  - Complex analysis â†’ DeepSeek API (cheapest)
  - Creative tasks â†’ OpenAI (best quality)
  - Technical coding â†’ DeepSeek Coder (specialized)
  - Multimodal needs â†’ Gemini (vision capabilities)
```

### Intelligent Cost Management
```python
class CostOptimizer:
    def __init__(self):
        self.monthly_budget = float(os.getenv("LLM_MONTHLY_BUDGET", "1000"))
        self.cost_tracker = {}
        self.provider_costs = {
            "local": 0.0,
            "openai": {"gpt-4o": 0.0025, "gpt-3.5-turbo": 0.0005},
            "deepseek": {"deepseek-chat": 0.00014, "deepseek-coder": 0.00014},
            "gemini": {"gemini-1.5-pro": 0.00125, "gemini-1.5-flash": 0.00035}
        }
        
    async def select_cost_optimal_provider(self, query: LLMRequest) -> str:
        current_spend = sum(self.cost_tracker.values())
        remaining_budget = self.monthly_budget - current_spend
        
        # Budget emergency - use local only
        if remaining_budget < self.monthly_budget * 0.1:
            return "local"
            
        # Calculate cost-benefit for each provider
        options = self.calculate_cost_benefit(query)
        
        # Select best option within budget
        for provider, score in sorted(options.items(), key=lambda x: x[1], reverse=True):
            estimated_cost = self.estimate_query_cost(query, provider)
            if estimated_cost <= remaining_budget * 0.05:  # Max 5% of remaining budget per query
                return provider
                
        return "local"  # Fallback to free option
```

## ðŸ“Š Performance Monitoring & Analytics

### LLM Performance Metrics
```yaml
Local LLM Monitoring:
  - Response time: <500ms target
  - Token throughput: 20-60 tokens/sec
  - Memory usage: <8GB sustained
  - GPU utilization: <90% sustained
  - Error rate: <1%
  
External API Monitoring:
  - Response time: <3s target
  - API availability: >99.9%
  - Cost per query: Track against budget
  - Rate limit utilization: <80%
  - Quality scores: User feedback based
  
System Health Checks:
  - Local model availability
  - API key validity and quotas
  - Network connectivity to API providers
  - Failover system responsiveness
```

### Hybrid Performance Dashboard
```python
# Performance Analytics
class LLMPerformanceMonitor:
    def __init__(self):
        self.metrics_store = TimescaleDB("llm_performance")
        
    async def track_request(self, request: LLMRequest, response: LLMResponse):
        await self.metrics_store.insert({
            "timestamp": datetime.utcnow(),
            "provider": response.provider,
            "model": response.model,
            "response_time_ms": response.response_time,
            "tokens_used": response.tokens_used,
            "cost_usd": response.cost,
            "quality_score": response.quality_score,
            "privacy_level": request.privacy_level,
            "query_complexity": request.complexity_score
        })
        
    async def generate_optimization_report(self) -> Dict:
        """Generate recommendations for LLM usage optimization"""
        
        metrics = await self.metrics_store.query("""
            SELECT 
                provider,
                AVG(response_time_ms) as avg_response_time,
                SUM(cost_usd) as total_cost,
                AVG(quality_score) as avg_quality,
                COUNT(*) as request_count
            FROM llm_performance 
            WHERE timestamp >= NOW() - INTERVAL '30 days'
            GROUP BY provider
        """)
        
        return {
            "cost_optimization": self.analyze_cost_efficiency(metrics),
            "performance_optimization": self.analyze_response_times(metrics),
            "quality_optimization": self.analyze_quality_scores(metrics),
            "recommendations": self.generate_recommendations(metrics)
        }
```

## ðŸ”§ Configuration Management

### Environment Configuration
```yaml
# .env configuration for hybrid LLM setup
LLM_STRATEGY=hybrid  # local, hybrid, api-only

# Local LLM Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama3.2:3b-instruct-q4_K_M
OLLAMA_KEEP_ALIVE=30m

# External API Configuration
OPENAI_API_KEY=sk-...
OPENAI_ORG_ID=org-...
DEEPSEEK_API_KEY=sk-...
GOOGLE_API_KEY=AIza...

# Cost Controls
LLM_MONTHLY_BUDGET=1000.0
COST_ALERT_THRESHOLD=0.8
EMERGENCY_LOCAL_ONLY=false

# Privacy Controls
DEFAULT_PRIVACY_LEVEL=PRIVATE
ALLOW_API_FOR_AGGREGATED=true
AUDIT_ALL_API_CALLS=true

# Performance Controls
LOCAL_TIMEOUT_SEC=10
API_TIMEOUT_SEC=30
MAX_CONCURRENT_LOCAL=2
MAX_CONCURRENT_API=10
```

### Dynamic Configuration
```python
# Runtime LLM configuration
class LLMConfig:
    def __init__(self):
        self.load_config()
        
    def load_config(self):
        self.strategy = os.getenv("LLM_STRATEGY", "hybrid")
        self.monthly_budget = float(os.getenv("LLM_MONTHLY_BUDGET", "1000"))
        self.default_privacy = os.getenv("DEFAULT_PRIVACY_LEVEL", "PRIVATE")
        
        # Provider availability check
        self.available_providers = self.check_provider_availability()
        
    async def check_provider_availability(self) -> Dict[str, bool]:
        """Check which LLM providers are currently available"""
        availability = {}
        
        # Local Ollama check
        try:
            response = await self.ollama_client.list_models()
            availability["local"] = len(response) > 0
        except:
            availability["local"] = False
            
        # API providers check
        for provider in ["openai", "deepseek", "gemini"]:
            availability[provider] = await self.check_api_availability(provider)
            
        return availability
        
    def get_optimal_provider(self, query: LLMRequest) -> str:
        """Get the optimal provider for this specific query"""
        
        # Privacy constraints
        if query.privacy_level == "PRIVATE":
            return "local" if self.available_providers["local"] else None
            
        # Performance requirements  
        if query.urgency == "HIGH":
            if self.available_providers["local"]:
                return "local"
            return "openai"  # Fastest API
            
        # Cost optimization
        if query.cost_sensitivity == "HIGH":
            if self.available_providers["local"]:
                return "local"
            return "deepseek"  # Cheapest API
            
        # Quality requirements
        if query.complexity_score > 0.8:
            return "openai"  # Best quality for complex tasks
            
        return "local"  # Default to local when possible
```

This hybrid LLM architecture provides maximum flexibility while maintaining privacy, cost control, and performance optimization for the EAIO energy management system. 

## ðŸš€ Technology Stack Overview

The EAIO system uses a **hybrid LLM architecture with MCP integration** and **LangGraph multi-agent framework**, optimized for both local privacy-first processing and advanced external API capabilities.

## Layer 4: Multi-Agent Framework

### LangGraph + LangChain Integration
```yaml
Primary Framework: LangGraph v0.2.x
  description: "State-based multi-agent workflow orchestration"
  advantages:
    - Native state management for complex workflows
    - Built-in checkpointing and recovery
    - Conditional edge routing for dynamic workflows
    - Integration with LangChain ecosystem
  
  core_features:
    - StateGraph: Define agent workflow topology
    - Conditional Edges: Dynamic routing based on state
    - Checkpointing: Persistent workflow state
    - Streaming: Real-time workflow execution updates

LangChain Framework: v0.3.x
  description: "Agent orchestration and tool integration"
  components:
    - ConversationBufferWindowMemory: Short-term conversation memory
    - AgentExecutor: Tool-enabled agent execution
    - BaseMessage: Structured message handling
    - StructuredChatAgent: JSON-structured agent responses
  
  memory_types:
    short_term:
      type: "ConversationBufferWindowMemory"
      window_size: 20  # Last 20 exchanges
      storage: "Redis (in-memory)"
      ttl: "1 hour"
      
    working_memory:
      type: "ConversationSummaryBufferMemory" 
      max_token_limit: 2000
      summarization_llm: "Llama-3.2-3B"
      storage: "Redis (persistent)"
      
    episodic_memory:
      type: "VectorStoreRetrieverMemory"
      vector_store: "Milvus"
      embedding_model: "all-MiniLM-L6-v2"
      retrieval_limit: 5
      similarity_threshold: 0.7

LangSmith Integration: v0.1.x
  description: "Agent performance monitoring and tracing"
  features:
    - Workflow Tracing: End-to-end execution tracking
    - Agent Performance: Response time and accuracy metrics
    - Cost Tracking: Token usage and API costs
    - Error Monitoring: Failure detection and analysis
    - A/B Testing: Compare agent configurations
  
  monitoring_setup:
    project_name: "eaio-multi-agent"
    environment: "production"
    trace_sampling: 100  # Trace all interactions
    metadata_capture:
      - agent_type
      - building_id  
      - user_role
      - response_time
      - token_usage
      - mcp_tools_used

Multi-Agent Configuration:
  coordinator_agent:
    llm_provider: "Local Llama-3.2-3B"
    memory_type: "working_memory"
    mcp_tools: ["all_categories"]
    role: "Workflow orchestration and routing"
    
  data_intelligence_agent:
    llm_provider: "Local Llama-3.2-3B" 
    memory_type: "episodic_memory"
    mcp_tools: ["data_collection", "analysis"]
    specialization: "BDG2 data analysis and pattern recognition"
    
  optimization_strategist_agent:
    llm_provider: "External DeepSeek-V3"
    memory_type: "working_memory + episodic_memory"
    mcp_tools: ["analysis", "recommendations"]
    specialization: "Energy optimization strategy development"
    
  forecast_intelligence_agent:
    llm_provider: "Hybrid routing"
    memory_type: "episodic_memory"
    mcp_tools: ["analysis", "weather_integration"]
    specialization: "Predictive energy consumption modeling"
    
  control_coordination_agent:
    llm_provider: "Local Qwen2.5-7B"
    memory_type: "short_term_memory"
    mcp_tools: ["control", "validation"]
    specialization: "Building system control and safety validation"
```

### Agent State Management
```python
# LangGraph Agent State Schema
class EAIOAgentState(TypedDict):
    # Core workflow state
    messages: List[BaseMessage]
    current_agent: str
    workflow_status: str
    
    # Building context
    building_context: Dict[str, Any]
    building_id: str
    user_role: str
    
    # Analysis results from specialists
    analysis_results: Dict[str, Dict]
    recommendations: List[Dict]
    forecasts: Dict[str, Any]
    control_commands: List[Dict]
    
    # Memory and conversation
    conversation_id: str
    memory_context: Dict[str, Any]
    
    # Performance tracking
    timestamp: float
    token_usage: Dict[str, int]
    mcp_tools_used: List[str]
    
    # Error handling
    errors: List[Dict]
    retry_count: int
    fallback_used: bool

# State transition validation
def validate_state_transition(from_agent: str, to_agent: str, state: EAIOAgentState) -> bool:
    """Validate agent state transitions"""
    
    transition_rules = {
        "coordinator": ["data_intelligence", "optimization_strategist", "forecast_intelligence", "control_coordination"],
        "data_intelligence": ["response_synthesizer", "optimization_strategist"], 
        "optimization_strategist": ["control_coordination", "response_synthesizer"],
        "forecast_intelligence": ["optimization_strategist", "response_synthesizer"],
        "control_coordination": ["response_synthesizer"],
        "response_synthesizer": []  # Terminal state
    }
    
    return to_agent in transition_rules.get(from_agent, [])
```

## Layer 5: Memory Systems

### Comprehensive Memory Architecture
```yaml
Memory Architecture: "Multi-layered memory system for agent intelligence"

Layer 1 - Short-term Memory (Working Memory):
  technology: "Redis + LangChain ConversationBufferWindowMemory"
  capacity: "20 conversation exchanges (last 1 hour)"
  purpose: "Immediate conversation context and agent coordination"
  storage_location: "Redis DB 2"
  ttl: "1 hour"
  
  implementation:
    conversation_tracking:
      format: "List[BaseMessage]"
      max_messages: 20
      auto_summarization: true
      
    agent_coordination:
      shared_state: "Current workflow status and intermediate results"
      message_passing: "Inter-agent communication buffer"
      error_context: "Recent errors and recovery attempts"

Layer 2 - Working Memory (Session Memory):
  technology: "Redis + LangChain ConversationSummaryBufferMemory"
  capacity: "2000 tokens of summarized context"
  purpose: "Extended session context with intelligent summarization"
  storage_location: "Redis DB 3" 
  ttl: "24 hours"
  
  implementation:
    summarization_llm: "Llama-3.2-3B (local)"
    summarization_trigger: "When token limit exceeded"
    context_preservation: "Key facts, user preferences, building specifics"
    
    session_context:
      user_preferences: "Analysis depth, preferred visualizations"
      building_focus: "Currently analyzed buildings and their context"
      optimization_goals: "User-specified energy targets and constraints"

Layer 3 - Episodic Memory (Long-term Patterns):
  technology: "Milvus + OpenAI Embeddings"
  capacity: "Unlimited (building-specific patterns and insights)"
  purpose: "Long-term building behavior patterns and optimization history"
  storage_location: "Milvus collection: building_episodic_memory"
  
  implementation:
    embedding_model: "all-MiniLM-L6-v2 (384 dimensions)"
    similarity_search: "Cosine similarity with threshold 0.7"
    metadata_fields:
      - building_id
      - timestamp
      - memory_type: ["pattern", "anomaly", "optimization", "insight"]
      - confidence_score
      - agent_source
      
    memory_types:
      patterns: "Recurring energy consumption behaviors"
      anomalies: "Unusual consumption events and their causes"
      optimizations: "Successful optimization strategies and results"
      insights: "AI-generated insights about building performance"

Layer 4 - Semantic Memory (Knowledge Base):
  technology: "Milvus + ChromaDB hybrid"
  capacity: "Building Domain Knowledge + BDG2 Dataset Insights"
  purpose: "Structured domain knowledge for intelligent reasoning"
  storage_location: "Milvus collection: domain_knowledge"
  
  implementation:
    knowledge_categories:
      building_systems: "HVAC, lighting, control system knowledge"
      energy_physics: "Thermodynamics, efficiency principles"
      bdg2_benchmarks: "Building performance benchmarks and comparisons"
      optimization_strategies: "Proven energy optimization techniques"
      
    knowledge_retrieval:
      query_expansion: "Automatic query expansion for better retrieval"
      context_ranking: "Relevance scoring based on building type and context"
      knowledge_fusion: "Combining multiple knowledge sources"

Layer 5 - Procedural Memory (Agent Skills):
  technology: "MCP Tool Registry + Skill Database"
  capacity: "Agent capabilities and learned procedures"
  purpose: "Store and retrieve agent skills, tool configurations, and procedures"
  storage_location: "PostgreSQL + JSON schema"
  
  implementation:
    skill_categories:
      analysis_procedures: "Step-by-step analysis workflows"
      optimization_algorithms: "Optimization strategies and parameters"
      control_procedures: "Building control sequences and safety checks"
      reporting_templates: "Structured report generation procedures"
      
    skill_learning:
      success_tracking: "Track successful procedure executions"
      parameter_optimization: "Learn optimal parameters for procedures"
      failure_analysis: "Learn from failed procedures and improve"
```

### Memory Integration Patterns
```python
class EAIOMemoryManager:
    """Comprehensive memory management for EAIO agents"""
    
    def __init__(self):
        # Initialize memory layers
        self.short_term = Redis(host='localhost', port=6379, db=2)
        self.working_memory = Redis(host='localhost', port=6379, db=3)
        self.episodic_memory = MilvusClient(uri="http://localhost:19530")
        self.semantic_memory = ChromaClient()
        self.procedural_memory = PostgreSQLClient()
        
        # Memory coordination
        self.memory_coordinator = MemoryCoordinator()
        
    async def get_context_for_agent(self, agent_type: str, building_id: str, conversation_id: str) -> Dict:
        """Retrieve appropriate memory context for specific agent"""
        
        context = {}
        
        # Short-term conversation context
        if agent_type in ["coordinator", "response_synthesizer"]:
            context["conversation"] = await self.get_short_term_memory(conversation_id)
            
        # Working memory for session context
        if agent_type != "control_coordination":  # Control agents need minimal context
            context["session"] = await self.get_working_memory(conversation_id)
            
        # Episodic memory for building-specific patterns
        if building_id:
            context["building_patterns"] = await self.get_episodic_memory(building_id, agent_type)
            
        # Semantic memory for domain knowledge
        context["domain_knowledge"] = await self.get_semantic_memory(agent_type)
        
        # Procedural memory for agent skills
        context["procedures"] = await self.get_procedural_memory(agent_type)
        
        return context
    
    async def update_memory_from_interaction(self, agent_type: str, building_id: str, 
                                           interaction_data: Dict, results: Dict):
        """Update memory layers based on agent interaction results"""
        
        # Update short-term memory
        await self.update_short_term_memory(
            interaction_data["conversation_id"],
            interaction_data["messages"]
        )
        
        # Update working memory with session insights
        if results.get("insights"):
            await self.update_working_memory(
                interaction_data["conversation_id"],
                results["insights"]
            )
            
        # Update episodic memory with building-specific learnings
        if building_id and results.get("patterns"):
            await self.update_episodic_memory(
                building_id,
                agent_type,
                results["patterns"]
            )
            
        # Update procedural memory with successful procedures
        if results.get("successful_procedures"):
            await self.update_procedural_memory(
                agent_type,
                results["successful_procedures"]
            )
    
    async def memory_consolidation(self, building_id: str):
        """Periodic memory consolidation to extract long-term patterns"""
        
        # Extract patterns from recent episodic memories
        recent_memories = await self.episodic_memory.search(
            collection_name="building_episodic_memory",
            filter=f'building_id == "{building_id}" and timestamp > {time.time() - 86400*7}',  # Last week
            limit=100
        )
        
        # Use local LLM to identify patterns
        pattern_extractor = LangChainAgent(
            llm=LocalLLM("Llama-3.2-3B"),
            task="Extract recurring patterns from building memory data"
        )
        
        consolidated_patterns = await pattern_extractor.analyze(recent_memories)
        
        # Store consolidated patterns
        await self.store_consolidated_patterns(building_id, consolidated_patterns)
```

## Integration Matrix

### Framework Integration Overview
| Component | Technology | Purpose | Integration Method |
|-----------|------------|---------|------------------|
| **Workflow Orchestration** | LangGraph StateGraph | Multi-agent coordination | Native state management |
| **Agent Framework** | LangChain Agents | Individual agent logic | Tool integration + memory |
| **Tool Integration** | MCP Protocol | External tool access | Standardized JSON-RPC |
| **Memory Management** | Multi-layer (Redis+Milvus) | Context and learning | Automatic memory updates |
| **Performance Monitoring** | LangSmith | Agent performance tracking | Workflow tracing |
| **LLM Routing** | Hybrid Router | Local + external LLMs | Privacy-aware routing |

### Technology Decision Matrix
| Requirement | Technology Choice | Alternative Considered | Decision Rationale |
|-------------|------------------|----------------------|-------------------|
| **Multi-Agent Framework** | LangGraph + LangChain | AutoGen, CrewAI | State management + LangChain ecosystem |
| **Memory System** | Multi-layer (Redis+Milvus) | Single vector store | Different memory needs require different solutions |
| **Tool Integration** | MCP Protocol | Direct API calls | Standardization and maintainability |
| **Monitoring** | LangSmith | Custom logging | Purpose-built for LangChain workflows |
| **Local LLM** | Ollama + vLLM | Direct model loading | Better resource management |

### Performance Targets with Framework
| Metric | Target | Technology Enabler |
|--------|--------|--------------------|
| **Agent Response Time** | < 5s (simple), < 30s (complex) | Local LLM + MCP caching |
| **Memory Retrieval** | < 200ms vector search | Milvus optimization |
| **Workflow Coordination** | < 100ms state transitions | LangGraph state management |
| **Tool Execution** | < 2s average | MCP server optimization |
| **Memory Updates** | < 50ms short-term | Redis in-memory operations |